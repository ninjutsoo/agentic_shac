{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Baseline Inference Engine\n",
        "\n",
        "This notebook tests the baseline LLM inference on sample data.\n",
        "\n",
        "**Tests:**\n",
        "1. Load baseline model\n",
        "2. Run inference on sample data\n",
        "3. Display predictions vs ground truth\n",
        "4. Compute basic metrics\n",
        "\n",
        "**Expected Output:**\n",
        "- Model loads successfully\n",
        "- Predictions generated for all samples\n",
        "- Metrics computed (accuracy, per-class stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "from src.baselines.llama_single import LlamaSingleBaseline\n",
        "from src.evaluation.metrics import compute_all_metrics, print_metrics_report\n",
        "from src.utils.preprocess import load_from_jsonl\n",
        "import yaml\n",
        "import torch\n",
        "\n",
        "print(\"✅ Imports successful\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load baseline config\n",
        "config_path = project_root / 'configs' / 'baseline.yaml'\n",
        "with open(config_path, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Baseline Configuration:\")\n",
        "print(f\"  Model: {config['model_name']}\")\n",
        "print(f\"  Dtype: {config['dtype']}\")\n",
        "print(f\"  4-bit: {config['load_in_4bit']}\")\n",
        "print(f\"  Max tokens: {config['max_new_tokens']}\")\n",
        "print(f\"  Temperature: {config['temperature']}\")\n",
        "print(f\"  Prompt template: {config['prompt_template']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Check GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Baseline Model\n",
        "\n",
        "This will take ~30-60 seconds to load the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline = LlamaSingleBaseline(config)\n",
        "baseline.load_model()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Memory after loading:\")\n",
        "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Sample Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed training data\n",
        "data_path = project_root / 'data' / 'processed' / 'train.jsonl'\n",
        "\n",
        "all_samples = load_from_jsonl(data_path)\n",
        "print(f\"Total training samples: {len(all_samples)}\")\n",
        "\n",
        "# Take 10 samples for testing\n",
        "n_samples = 10\n",
        "samples = all_samples[:n_samples]\n",
        "print(f\"\\nTesting on {len(samples)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Display Sample Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Sample 1:\")\n",
        "print(f\"  Text: {samples[0]['text'][:150]}...\")\n",
        "print(f\"  Trigger: {samples[0]['trigger_text']}\")\n",
        "print(f\"  True label: {samples[0]['status_label']}\")\n",
        "print(f\"  Source: {samples[0]['source']}\")\n",
        "\n",
        "print(\"\\nSample 2:\")\n",
        "print(f\"  Text: {samples[1]['text'][:150]}...\")\n",
        "print(f\"  Trigger: {samples[1]['trigger_text']}\")\n",
        "print(f\"  True label: {samples[1]['status_label']}\")\n",
        "print(f\"  Source: {samples[1]['source']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run Baseline Inference\n",
        "\n",
        "This will take ~2-3 seconds per sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = baseline.predict_batch(samples, show_progress=True)\n",
        "print(f\"\\n✅ Inference completed on {len(results)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Display Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "correct = 0\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    true_label = result['status_label']\n",
        "    pred_label = result['pred_label']\n",
        "    pred_letter = result['pred_letter']\n",
        "    match = \"✅\" if true_label == pred_label else \"❌\"\n",
        "    \n",
        "    if true_label == pred_label:\n",
        "        correct += 1\n",
        "    \n",
        "    print(f\"\\nSample {i}:\")\n",
        "    print(f\"  Text: {result['text'][:100]}...\")\n",
        "    print(f\"  Trigger: {result['trigger_text']}\")\n",
        "    print(f\"  True: {true_label}\")\n",
        "    print(f\"  Pred: {pred_label} ({pred_letter})\")\n",
        "    print(f\"  {match}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Quick Accuracy: {correct}/{len(results)} = {correct/len(results):.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compute Detailed Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true = [r['status_label'] for r in results]\n",
        "y_pred = [r['pred_label'] for r in results]\n",
        "\n",
        "labels = ['none', 'current', 'past', 'Not Applicable']\n",
        "metrics = compute_all_metrics(y_true, y_pred, labels=labels)\n",
        "\n",
        "print_metrics_report(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Validation Checklist\n",
        "\n",
        "✅ **Check these:**\n",
        "1. Model loads without errors\n",
        "2. GPU memory usage is reasonable (< 10GB for bf16)\n",
        "3. Inference completes for all samples\n",
        "4. Predictions are valid labels (none/current/past/Not Applicable)\n",
        "5. Some predictions match ground truth (>0% accuracy)\n",
        "6. Confusion matrix makes sense (no all-zero rows/columns)\n",
        "\n",
        "**Expected Performance (baseline, untrained):**\n",
        "- Accuracy: Variable, typically 20-50% on small samples\n",
        "- FPR: May be high (model predicts 'current' too often)\n",
        "- This is expected - we'll improve it with the agentic approach!\n",
        "\n",
        "---\n",
        "\n",
        "**If all checks pass, proceed to Phase 3!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
