{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Notebook 04: Model Loading\n",
        "\n",
        "**Purpose**: Verify model loads and runs inference\n",
        "\n",
        "**Tests**:\n",
        "1. Load Llama-3.1-8B-Instruct model\n",
        "2. Check GPU memory usage\n",
        "3. Test inference on 1-2 sample prompts\n",
        "4. Verify output format (a|b|c|d)\n",
        "5. Test with different temperature settings\n",
        "6. Measure inference speed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from src.agentic.prompts import get_prompt, format_for_llama, parse_model_output, letter_to_label\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "print(\"GPU Availability:\")\n",
        "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    print(f\"  Current allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"  WARNING: No GPU available! Model loading will fail or be very slow.\")\n",
        "\n",
        "# Load config\n",
        "with open('../configs/baseline.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(f\"\\nBaseline Configuration:\")\n",
        "print(f\"  Model: {config['model_name']}\")\n",
        "print(f\"  dtype: {config['dtype']}\")\n",
        "print(f\"  Load in 4-bit: {config['load_in_4bit']}\")\n",
        "print(f\"  Temperature: {config['temperature']}\")\n",
        "print(f\"  Max new tokens: {config['max_new_tokens']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Model and Tokenizer\n",
        "\n",
        "This will take a few minutes and ~16GB GPU memory for bf16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "\n",
        "print(\"Loading model... (this may take a few minutes)\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load model with appropriate dtype\n",
        "if config['load_in_4bit']:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config['model_name'],\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        config['model_name'],\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16 if config['dtype'] == 'bf16' else torch.float16\n",
        "    )\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n✅ Model loaded in {load_time:.1f}s\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Inference on Sample Prompts\n",
        "\n",
        "Create sample prompts and test model inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"note\": \"Social History: Tob (-), EtOH - a glass of wine 1-2x/month, IVDU (-), lives with her husband.\",\n",
        "        \"trigger\": \"IVDU\",\n",
        "        \"expected\": \"none\"\n",
        "    },\n",
        "    {\n",
        "        \"note\": \"Patient admits to daily heroin use by injection. Currently using 2-3 bags per day.\",\n",
        "        \"trigger\": \"heroin\",\n",
        "        \"expected\": \"current\"\n",
        "    },\n",
        "    {\n",
        "        \"note\": \"History of cocaine abuse in 1990s, has been clean for 20 years.\",\n",
        "        \"trigger\": \"cocaine\", \n",
        "        \"expected\": \"past\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Create prompts\n",
        "for i, case in enumerate(test_cases):\n",
        "    prompt_dict = get_prompt(\"status_v1\", note=case[\"note\"], trigger=case[\"trigger\"])\n",
        "    formatted = format_for_llama(prompt_dict[\"system\"], prompt_dict[\"user\"])\n",
        "    \n",
        "    print(f\"\\nTest Case {i+1}:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Note: {case['note'][:100]}...\")\n",
        "    print(f\"Trigger: '{case['trigger']}'\")\n",
        "    print(f\"Expected: {case['expected']}\")\n",
        "    print(f\"\\nFormatted prompt length: {len(formatted)} chars\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run inference on test cases\n",
        "print(\"Running inference...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, case in enumerate(test_cases):\n",
        "    prompt_dict = get_prompt(\"status_v1\", note=case[\"note\"], trigger=case[\"trigger\"])\n",
        "    formatted = format_for_llama(prompt_dict[\"system\"], prompt_dict[\"user\"])\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=config['max_new_tokens'],\n",
        "            temperature=config['temperature'],\n",
        "            top_p=config['top_p'],\n",
        "            do_sample=True if config['temperature'] > 0 else False\n",
        "        )\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract response (remove prompt)\n",
        "    response = generated_text[len(formatted):].strip()\n",
        "    \n",
        "    # Parse output\n",
        "    letter = parse_model_output(response)\n",
        "    predicted_label = letter_to_label(letter) if letter else \"PARSE_ERROR\"\n",
        "    \n",
        "    # Display results\n",
        "    print(f\"\\nTest Case {i+1}:\")\n",
        "    print(f\"  Trigger: '{case['trigger']}'\")\n",
        "    print(f\"  Expected: {case['expected']}\")\n",
        "    print(f\"  Model output: '{response[:50]}'\")\n",
        "    print(f\"  Parsed letter: {letter}\")\n",
        "    print(f\"  Predicted label: {predicted_label}\")\n",
        "    print(f\"  ✓ CORRECT\" if predicted_label == case['expected'] else f\"  ✗ INCORRECT\")\n",
        "    print(f\"  Inference time: {inference_time:.3f}s\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Validation Checklist\n",
        "\n",
        "**Check before proceeding:**\n",
        "\n",
        "- Model loads successfully on GPU\n",
        "- GPU memory usage reasonable (<20GB for bf16)\n",
        "- Inference runs without errors on sample data\n",
        "- Output parsing (a|b|c|d → labels) works correctly\n",
        "- Model produces sensible outputs for clinical notes\n",
        "- Inference speed acceptable (~0.2-0.5s per sample)\n",
        "\n",
        "If all checks pass, proceed to implementing the full baseline inference engine!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
