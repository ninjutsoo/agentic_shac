{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Notebook 02: Preprocessing Pipeline\n",
        "\n",
        "**Purpose**: Validate preprocessing on sample data\n",
        "\n",
        "**Tests**:\n",
        "1. Load sample parsed data\n",
        "2. Show text cleaning (before/after)\n",
        "3. Display processed JSONL format\n",
        "4. Verify data integrity (no lost samples)\n",
        "5. Check for any edge cases (empty text, special characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from pathlib import Path\n",
        "from src.utils.brat_loader import BRATLoader\n",
        "from src.utils.preprocess import clean_text, process_events, save_to_jsonl, load_from_jsonl, get_split_statistics\n",
        "import yaml\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Sample Data\n",
        "\n",
        "Load 10 files to test preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load config\n",
        "with open('../configs/data.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Load sample data from first 5 files per source\n",
        "loader = BRATLoader(target_event=\"Drug\")\n",
        "sample_events = []\n",
        "\n",
        "for source in ['mimic', 'uw']:\n",
        "    dir_path = Path(config['raw_root']) / 'train' / source\n",
        "    txt_files = sorted(dir_path.glob('*.txt'))[:5]\n",
        "    \n",
        "    for txt_file in txt_files:\n",
        "        ann_file = txt_file.with_suffix('.ann')\n",
        "        \n",
        "        with open(txt_file, 'r') as f:\n",
        "            text = f.read()\n",
        "        \n",
        "        ann_data = loader.parse_ann_file(ann_file)\n",
        "        events = loader.extract_drug_events(\n",
        "            ann_data=ann_data,\n",
        "            text=text,\n",
        "            note_id=txt_file.stem,\n",
        "            source=source,\n",
        "            split='train'\n",
        "        )\n",
        "        sample_events.extend(events)\n",
        "\n",
        "print(f\"Loaded {len(sample_events)} Drug events from 10 sample files\")\n",
        "print(f\"Sample event keys: {list(sample_events[0].keys()) if sample_events else 'None'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Text Cleaning\n",
        "\n",
        "Show before/after text cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show text cleaning examples\n",
        "if sample_events:\n",
        "    for i, event in enumerate(sample_events[:3]):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(f\"BEFORE cleaning ({len(event['text'])} chars):\")\n",
        "        print(repr(event['text'][:200]))\n",
        "        \n",
        "        cleaned = clean_text(event['text'])\n",
        "        print(f\"\\nAFTER cleaning ({len(cleaned)} chars):\")\n",
        "        print(repr(cleaned[:200]))\n",
        "        print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Process and Save Sample Data\n",
        "\n",
        "Test JSONL save/load functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process events\n",
        "processed = process_events(sample_events, clean=True)\n",
        "print(f\"Processed {len(processed)}/{len(sample_events)} events\")\n",
        "\n",
        "# Save to test file\n",
        "test_output = Path('../data/processed/test_sample.jsonl')\n",
        "save_to_jsonl(processed, test_output)\n",
        "\n",
        "# Load back\n",
        "loaded = load_from_jsonl(test_output)\n",
        "print(f\"Loaded {len(loaded)} events from JSONL\")\n",
        "\n",
        "# Verify integrity\n",
        "print(f\"\\n✅ Data integrity check:\")\n",
        "print(f\"  Original: {len(sample_events)} events\")\n",
        "print(f\"  Processed: {len(processed)} events\")\n",
        "print(f\"  Loaded: {len(loaded)} events\")\n",
        "print(f\"  Match: {len(processed) == len(loaded)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ Validation Checklist\n",
        "\n",
        "**Check before proceeding:**\n",
        "\n",
        "- Text cleaning works (normalizes whitespace)\n",
        "- JSONL save/load works correctly\n",
        "- No data lost in processing\n",
        "- All fields preserved correctly\n",
        "- Sample file created successfully\n",
        "\n",
        "If all checks pass, proceed to full dataset processing!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
